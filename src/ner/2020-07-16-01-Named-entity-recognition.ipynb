{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/componavt/topkar-space/blob/main/src/ner/2020-07-16-01-Named-entity-recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOYQatTUP4X"
      },
      "source": [
        "# Named-entity recognition\n",
        "> This chapter will introduce a slightly more advanced topic - named-entity recognition. You'll learn how to identify the who, what, and where of your texts using pre-trained models on English and non-English text. You'll also learn how to use some new libraries, polyglot and spaCy, to add to your NLP toolbox. This is the Summary of lecture \"Introduction to Natural Language Processing in Python\", via datacamp.\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Chanseok Kang\n",
        "- categories: [Python, Datacamp, Natural_Language_Processing]\n",
        "- image: images/chunk_pie.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qvgJssNhUP4Y"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y34y4sdkUP4Z"
      },
      "source": [
        "## Named Entity Recognition\n",
        "- Named Entity Recognition (NER)\n",
        "    - NLP task to identify important named entities in the text\n",
        "        - People, places, organizations\n",
        "        - Dates, states, works of art\n",
        "    - Can be used alongside topic identification\n",
        "    - Who? What? When? Where?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IOr021NUP4a"
      },
      "source": [
        "### NER with NLTK\n",
        "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use `nltk` to find the named entities in this article.\n",
        "\n",
        "What might the article be about, given the names you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reUOJ8lWUP4a"
      },
      "source": [
        "> Note: Before using NER through NLTK, you must install 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words' packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0INBYroOUP4b",
        "outputId": "8d846f0d-261c-460e-c023-01ca0e6e76c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "OVeGVq74UP4c",
        "outputId": "c083eab0-8247-4071-cfe6-f3f007d91eef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index               Name            Synonym  \\\n",
              "0      0             Нулица             нулица   \n",
              "1      1   Карельская пожня   карельская пожня   \n",
              "2      2  Наволоцкая поляна  наволоцкая поляна   \n",
              "3      3    Галайские пожни    галайские пожни   \n",
              "4      4          Pahag’ärv           pahagärv   \n",
              "\n",
              "                                                Text  \n",
              "0  Река, находится в 300 м от д.Типиницы на запад...  \n",
              "1  Покос на берегу Виговской губы, между Педейнав...  \n",
              "2                Пахотная поляна в устье Пижейручья.  \n",
              "3  покосы на юге острова Галайский, см. Галайский...  \n",
              "4  Маленькое озерко за хутором Poh'd'ad'g', между...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0852f60c-5ab5-4868-9fa5-8e4eab545628\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Name</th>\n",
              "      <th>Synonym</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Нулица</td>\n",
              "      <td>нулица</td>\n",
              "      <td>Река, находится в 300 м от д.Типиницы на запад...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Карельская пожня</td>\n",
              "      <td>карельская пожня</td>\n",
              "      <td>Покос на берегу Виговской губы, между Педейнав...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Наволоцкая поляна</td>\n",
              "      <td>наволоцкая поляна</td>\n",
              "      <td>Пахотная поляна в устье Пижейручья.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Галайские пожни</td>\n",
              "      <td>галайские пожни</td>\n",
              "      <td>покосы на юге острова Галайский, см. Галайский...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Pahag’ärv</td>\n",
              "      <td>pahagärv</td>\n",
              "      <td>Маленькое озерко за хутором Poh'd'ad'g', между...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0852f60c-5ab5-4868-9fa5-8e4eab545628')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0852f60c-5ab5-4868-9fa5-8e4eab545628 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0852f60c-5ab5-4868-9fa5-8e4eab545628');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc254d52-01f1-4f06-b2a6-c76f812d0b74\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc254d52-01f1-4f06-b2a6-c76f812d0b74')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc254d52-01f1-4f06-b2a6-c76f812d0b74 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\u0414\\u0430\\u043b\\u044c\\u043d\\u0435\\u0435 \\u043f\\u043e\\u043b\\u0435\",\n          \"\\u041a\\u0430\\u0440\\u0435\\u043b\\u044c\\u0441\\u043a\\u0430\\u044f \\u043f\\u043e\\u0436\\u043d\\u044f\",\n          \"Sod\\u2019j\\u00e4rvt\\u2019e\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Synonym\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\u0434\\u0430\\u043b\\u044c\\u043d\\u0435\\u0435 \\u043f\\u043e\\u043b\\u0435\",\n          \"\\u043a\\u0430\\u0440\\u0435\\u043b\\u044c\\u0441\\u043a\\u0430\\u044f \\u043f\\u043e\\u0436\\u043d\\u044f\",\n          \"sodj\\u00e4rvte\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"\\u0420\\u0435\\u043a\\u0430 \\u041a\\u043e\\u043b\\u043f\\u044c, \\u0432\\u044b\\u0442\\u0435\\u043a\\u0430\\u0435\\u0442 \\u0438\\u0437 \\u043e\\u0437. Jok\\u0161ar'v, \\u0442\\u0435\\u0447\\u0435\\u0442 \\u0432 \\u0412\\u043e\\u043b\\u043e\\u0433\\u043e\\u0434\\u0441\\u043a. \\u043e\\u0431\\u043b.\",\n          \"\\u041f\\u043e\\u043a\\u043e\\u0441 \\u043d\\u0430 \\u0431\\u0435\\u0440\\u0435\\u0433\\u0443 \\u0412\\u0438\\u0433\\u043e\\u0432\\u0441\\u043a\\u043e\\u0439 \\u0433\\u0443\\u0431\\u044b, \\u043c\\u0435\\u0436\\u0434\\u0443 \\u041f\\u0435\\u0434\\u0435\\u0439\\u043d\\u0430\\u0432\\u043e\\u043b\\u043e\\u043a\\u043e\\u043c \\u0438 \\u0443\\u0441\\u0442\\u044c\\u0435\\u043c \\u041b\\u0435\\u0439\\u0440\\u0435\\u043a\\u0438. \\u041f\\u0440\\u0438\\u043d\\u0430\\u0434\\u043b\\u0435\\u0436\\u0430\\u043b \\u0441\\u0435\\u043c\\u044c\\u0435 \\u043d\\u0430\\u0438\\u0431\\u043e\\u043b\\u0435\\u0435 \\u0431\\u043e\\u0433\\u0430\\u0442\\u044b\\u0445 \\u043a\\u0443\\u043f\\u0446\\u043e\\u0432 \\u041a\\u043e\\u0440\\u0435\\u043b\\u044c\\u0441\\u043a\\u0438\\u0445. \\u041f\\u043e\\u0441\\u0435\\u043b\\u043e\\u043a \\u043a\\u0443\\u043f\\u0446\\u0430 \\u041a\\u043e\\u0440\\u0435\\u043b\\u044c\\u0441\\u043a\\u043e\\u0433\\u043e \\u043f\\u0440\\u0438 \\u0434.\\u0412\\u0438\\u0433\\u043e\\u0432\\u043e.\",\n          \"\\u041f\\u043e\\u043b\\u044f\\u043d\\u044b \\u043f\\u043e \\u0434\\u043e\\u0440\\u043e\\u0433\\u0435 \\u0432 \\u0434\\u0435\\u0440. \\u0421\\u0438\\u0434\\u043e\\u0440\\u043e\\u0432\\u043e.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_files = [\n",
        "    \"https://raw.githubusercontent.com/componavt/topkar-space/main/data/sample10.csv\",\n",
        "]\n",
        "\n",
        "df = pd.concat([pd.read_csv(url, sep = ';') for url in csv_files], ignore_index=True)\n",
        "df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "vdjrWRyXUP4d",
        "outputId": "748f4f7b-eafa-43b0-a348-2e17032d9408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2==0.9.1 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2==0.9.1) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2==0.9.1) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2==0.9.1) (0.6.2)\n",
            "\n",
            "Number of toponyms: 10\n",
            "\n",
            "sentences =  ['Река, находится в 300 м от д.Типиницы на запад.Вытекает из болота у д. Холмы, впадает в Типиницкий залив.']\n",
            "pos_tokens =  [('Река', 'NN'), (',', ','), ('находится', 'NNP'), ('в', 'VBZ'), ('300', 'CD'), ('м', 'NN'), ('от', 'NNP'), ('д.Типиницы', 'NNP'), ('на', 'NNP'), ('запад.Вытекает', 'NNP'), ('из', 'NNP'), ('болота', 'NNP'), ('у', 'NNP'), ('д.', 'NNP'), ('Холмы', 'NNP'), (',', ','), ('впадает', 'NNP'), ('в', 'NNP'), ('Типиницкий', 'NNP'), ('залив', 'NNP'), ('.', '.')]\n",
            "chunked_tokens =  <generator object ParserI.parse_sents.<locals>.<genexpr> at 0x7d929ab9af10>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "string index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d45169de3450>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;31m# print(\"lemma = \", lemma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunked_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/parse/api.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Use our feature detector to get the featureset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Use the classifier to pick a tag.  If a cutoff probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mfeature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \"\"\"\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m_feature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplify_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprevword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevprevword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')          # nltk.ne_chunk_sents\n",
        "\n",
        "# import string\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "\n",
        "!pip install -U pymorphy2==0.9.1\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
        "\n",
        "# Display\n",
        "# print(\"df (toponyms):\")\n",
        "# print(df.head().to_string())\n",
        "# print(df.describe())\n",
        "\n",
        "# Convert quotes to list\n",
        "lines = df['Text'].tolist()\n",
        "print()\n",
        "print(\"Number of toponyms:\", len(lines))\n",
        "\n",
        "#df['Text'] = df['Text'].replace(np.nan, '')\n",
        "df['Text'] = df['Text'].replace({float('nan'): \"\"})# replace empty text with \"None\"\n",
        "\n",
        "def extract_entity_names(t):\n",
        "  entity_names = []\n",
        "\n",
        "  if hasattr(t, 'label') and t.label() == 'NE':\n",
        "    entity_names.append(' '.join([child[0] for child in t]))\n",
        "  else:\n",
        "    for child in t:\n",
        "      entity_names.extend(extract_entity_names(child))\n",
        "\n",
        "  return entity_names\n",
        "\n",
        "for i, li in enumerate(lines):\n",
        "#for li in lines:\n",
        "    # Tokenize the text line into sentences: sentences\n",
        "    sentences = sent_tokenize(li, language='russian')\n",
        "    print(\"\\nsentences = \", sentences)\n",
        "\n",
        "    for sent in sentences:#                                                                universal, wsj, brown\n",
        "      tokens = nltk.word_tokenize(sent, language='russian', preserve_line=True)# , tagset='universal')\n",
        "\n",
        "      # Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "      pos_tokens = nltk.pos_tag(tokens)\n",
        "      #pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "      print(\"pos_tokens = \", pos_tokens)\n",
        "\n",
        "      chunked_tokens = nltk.ne_chunk_sents(pos_tokens, binary=True)\n",
        "      print(\"chunked_tokens = \", chunked_tokens)\n",
        "\n",
        "#      for word in tokens:\n",
        "        # no num and comma\n",
        "#        if (word.isalpha()) and (not word in stop_words):\n",
        "#          lemma = morph.parse(word)[0].normal_form\n",
        "          # print(\"lemma = \", lemma)\n",
        "\n",
        "      for ch in chunked_tokens:\n",
        "        if hasattr(ch, \"label\") and ch.label() == 'NE':\n",
        "          print(ch)\n",
        "\n",
        "    if i > 2:\n",
        "      break\n",
        "\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "#token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "# todo see https://stackoverflow.com/a/37869704/1173350\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "#pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "# chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "#for sent in chunked_sentences:\n",
        "#    for chunk in sent:\n",
        "#        if hasattr(chunk, \"label\") and chunk.label() == 'NE':\n",
        "#            print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HucHhhtNUP4f"
      },
      "source": [
        "### Charting practice\n",
        "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
        "\n",
        "You'll use a `defaultdict` called `ner_categories`, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called `chunked_sentences` similar to the last exercise, but this time with non-binary category names.\n",
        "\n",
        "You can use `hasattr()` to determine if each chunk has a 'label' and then simply use the chunk's `.label()` method as the dictionary key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARMbNKlJUP4g"
      },
      "outputs": [],
      "source": [
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSyFFy8-UP4g"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "\n",
        "# Create a list from the dictionary keys for the cart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(l) for l in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW2v3ORkUP4h"
      },
      "source": [
        "## Introduction to SpaCy\n",
        "- SpaCy\n",
        "    - NLP library similar to `gensim`, with different implementations\n",
        "    - Focuson creating NLP pipelines to generate models and corpora\n",
        "    - Open source, with extra libraries and tools\n",
        "        - Displacy\n",
        "- Why use SpaCy for NER?\n",
        "    - Easy pipeline creation\n",
        "    - Different entity types compared to `nltk`\n",
        "    - Informal language corpora\n",
        "        - Easily find entities in Tweets and chat messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0I85vBUP4i"
      },
      "source": [
        "### Comparing NLTK with spaCy NER\n",
        "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
        "\n",
        "> Note: To use english model in SpaCy, you need to install appropriate english model:\n",
        "```bash\n",
        "python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRVFqiu1UP4i"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = spacy.load('en_core_web_sm', tagger=False, parser=False, matcher=False)\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = nlp(article)\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sc_2jrSUP4i"
      },
      "source": [
        "From NER in `nltk`, we can get `organization`, `GPE`, `person` as a category. In case of `spacy`, we can additionally get `FAC`, `CARDINAL`, `LOC`, `MONEY` as a category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTUQy8fqUP4k"
      },
      "source": [
        "## Multilingual NER with polyglot\n",
        "- polyglot\n",
        "    - NLP library which uses word vectors\n",
        "    - vectors for many different languages (more than 130)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZhKxkOBUP4k"
      },
      "source": [
        "### French NER with polyglot I\n",
        "In this exercise and the next, you'll use the `polyglot` library to identify French entities. The library functions slightly differently than `spacy`, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
        "\n",
        "> Note: Before using polyglot for specific language, you need to download appropriate language model. Also, it requires some dependencies.\n",
        "```\n",
        "pip install pyicu\n",
        "pip install pycld2\n",
        "pip install morfessor\n",
        "polyglot download ner2.fr\n",
        "polyglot download embeddings2.fr\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_df8n42XUP4l"
      },
      "outputs": [],
      "source": [
        "!polyglot download ner2.fr\n",
        "!polyglot download embeddings2.fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0oXTi-OUP4l"
      },
      "outputs": [],
      "source": [
        "with open('./dataset/news_articles/french.txt', 'r') as file:\n",
        "    article = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXZ--B3xUP4l"
      },
      "outputs": [],
      "source": [
        "from polyglot.text import Text\n",
        "\n",
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "\n",
        "# Print the type of ent\n",
        "print(type(ent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvRxMcYBUP4m"
      },
      "source": [
        "### French NER with polyglot II\n",
        "Here, you'll complete the work you began in the previous exercise.\n",
        "\n",
        "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6grzNJ2UP4n"
      },
      "outputs": [],
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "pprint(entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwuRaDFYUP4n"
      },
      "source": [
        "### Spanish NER with polyglot\n",
        "You'll continue your exploration of `polyglot` now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
        "\n",
        "Your specific task is to determine how many of the entities contain the words `\"Márquez\"` or `\"Gabo\"` - these refer to the same person in different ways!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_F85X5CUP4o"
      },
      "outputs": [],
      "source": [
        "!polyglot download ner2.es embeddings2.es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B2lOFkbUP4o"
      },
      "outputs": [],
      "source": [
        "with open('./dataset/news_articles/spanish.txt', 'r') as file:\n",
        "    article = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1gbGDNBUP4o"
      },
      "outputs": [],
      "source": [
        "txt = Text(article)\n",
        "\n",
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "for ent in txt.entities:\n",
        "    # check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    if ('Márquez' in ent) or ('Gabo' in ent):\n",
        "        # Increment count\n",
        "        count += 1\n",
        "\n",
        "# Print count\n",
        "print(count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(percentage)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}